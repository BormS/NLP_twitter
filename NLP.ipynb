{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#Data Preprocessing and Feature Engineering\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "\n",
    "#Model Selection and Validation\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "from nltk.metrics import precision, recall, f_measure, ConfusionMatrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_tweet(tweet):\n",
    "    '''\n",
    "    Removes unsolicited symbols, stop words\n",
    "    Completes stemming \n",
    "    '''\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english') + ['AT_USER','URL'])\n",
    "    \n",
    "    tweet = ' '.join(word.strip(string.punctuation) for word in tweet.split())\n",
    "    tweet = tweet.lower() # convert text to lower-case\n",
    "    tweet = re.sub('((www[^\\s]+)|(http[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "    tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'', tweet) # remove the # in #hashtag\n",
    "    tweet = word_tokenize(tweet)\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    clean_tweets = []\n",
    "    for word in tweet:\n",
    "        if (word not in stopwords and # remove stopwords\n",
    "            word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = ps.stem(word) # stemming word\n",
    "            clean_tweets.append(stem_word)\n",
    "    return clean_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Byas Classifier without additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cteate the test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tweet):\n",
    "    '''\n",
    "    Create a bag of words\n",
    "    '''\n",
    "    words = preprocessed_tweet(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    true_data = data[data.target == 1]\n",
    "    true_data = true_data.text\n",
    "\n",
    "    false_data = data[data.target == 0]\n",
    "    false_data = false_data.text\n",
    "\n",
    "    true_tweets_set = []\n",
    "    for tweet in true_data:\n",
    "        true_tweets_set.append((bag_of_words(tweet), 1))\n",
    "\n",
    "    false_tweets_set = []\n",
    "    for tweet in false_data:\n",
    "        false_tweets_set.append((bag_of_words(tweet), 0))\n",
    "        \n",
    "    shuffle(true_tweets_set)\n",
    "    shuffle(false_tweets_set)\n",
    "\n",
    "    false_tweets_set = false_tweets_set[:3271]\n",
    "\n",
    "    size = int(0.2 * len(false_tweets_set))\n",
    "\n",
    "    test_set = true_tweets_set[:size] + false_tweets_set[:size]\n",
    "    train_set = true_tweets_set[size:] + false_tweets_set[size:]\n",
    "    return test_set, train_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, train_set = train_test_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7691131498470948\n",
      "Most Informative Features\n",
      "               hiroshima = True                1 : 0      =     40.3 : 1.0\n",
      "                  bomber = True                1 : 0      =     31.0 : 1.0\n",
      "                    atom = True                1 : 0      =     28.3 : 1.0\n",
      "                 wildfir = True                1 : 0      =     28.2 : 1.0\n",
      "                   sever = True                1 : 0      =     27.7 : 1.0\n",
      "                 suspect = True                1 : 0      =     23.7 : 1.0\n",
      "                 typhoon = True                1 : 0      =     23.7 : 1.0\n",
      "                outbreak = True                1 : 0      =     21.7 : 1.0\n",
      "                      70 = True                1 : 0      =     20.3 : 1.0\n",
      "                 airport = True                1 : 0      =     18.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy) \n",
    " \n",
    "print (classifier.show_most_informative_features(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics of the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(test_set):\n",
    "    actual_set = defaultdict(set)\n",
    "    predicted_set = defaultdict(set)\n",
    " \n",
    "    actual_set_cm = []\n",
    "    predicted_set_cm = []\n",
    " \n",
    "    for index, (feature, actual_label) in enumerate(test_set):\n",
    "        actual_set[actual_label].add(index)\n",
    "        actual_set_cm.append(actual_label)\n",
    " \n",
    "        predicted_label = classifier.classify(feature)\n",
    " \n",
    "        predicted_set[predicted_label].add(index)\n",
    "        predicted_set_cm.append(predicted_label)\n",
    "    \n",
    "\n",
    "    print ('Metrics for class True')\n",
    "    print ('true precision: ', precision(actual_set[1], predicted_set[1]))\n",
    "    print ('true recall:', recall(actual_set[1], predicted_set[1]))\n",
    "    print ('true F-measure:', f_measure(actual_set[1], predicted_set[1]))\n",
    "\n",
    "    print ('\\nMetrics for class False ')\n",
    "    print ('false precision:', precision(actual_set[0], predicted_set[0]))\n",
    "    print ('false recall:', recall(actual_set[0], predicted_set[0])) \n",
    "    print ('false F-measure:', f_measure(actual_set[0], predicted_set[0])) \n",
    "    print ('\\nConfusion Matrix ')\n",
    "    cm = ConfusionMatrix(actual_set_cm, predicted_set_cm)\n",
    "    print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for class True\n",
      "true precision:  0.7658610271903323\n",
      "true recall: 0.7752293577981652\n",
      "true F-measure: 0.770516717325228\n",
      "\n",
      "Metrics for class False \n",
      "false precision: 0.7724458204334366\n",
      "false recall: 0.7629969418960245\n",
      "false F-measure: 0.7676923076923077\n",
      "\n",
      "Confusion Matrix \n",
      "  |   0   1 |\n",
      "--+---------+\n",
      "0 |<499>155 |\n",
      "1 | 147<507>|\n",
      "--+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_metrics(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final F-scores are equal to 0.77 and 0.76 for the classes True and False respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part during the preprocessing symbol of # and links were removed. Now the model will be runed without removing this features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_tweet_evoluated(tweet):\n",
    "    '''\n",
    "    Removes unsolicited symbols, stop words\n",
    "    Completes stemming \n",
    "    '''\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    tweet = ' '.join(word.strip(string.punctuation) for word in tweet.split())\n",
    "    tweet = tweet.lower() # convert text to lower-case\n",
    "    tweet = re.sub('((www[^\\s]+)|(http[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "    tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "    tweet = re.sub(r'#([^\\s]+)', r' hashtag', tweet) # remove the # in #hashtag\n",
    "    tweet = word_tokenize(tweet)\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    clean_tweets = []\n",
    "    for word in tweet:\n",
    "        if (word not in stopwords and # remove stopwords\n",
    "            word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = ps.stem(word) # stemming word\n",
    "            clean_tweets.append(stem_word)\n",
    "            \n",
    "    return clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_evoluated(tweet):\n",
    "    '''\n",
    "    Create a bag of words\n",
    "    '''\n",
    "    words = preprocessed_tweet_evoluated(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_evoluated(data):\n",
    "    true_data = data[data.target == 1]\n",
    "    true_data = true_data.text\n",
    "\n",
    "    false_data = data[data.target == 0]\n",
    "    false_data = false_data.text\n",
    "\n",
    "    true_tweets_set = []\n",
    "    for tweet in true_data:\n",
    "        true_tweets_set.append((bag_of_words_evoluated(tweet), 1))\n",
    "\n",
    "    false_tweets_set = []\n",
    "    for tweet in false_data:\n",
    "        false_tweets_set.append((bag_of_words_evoluated(tweet), 0))\n",
    "        \n",
    "    shuffle(true_tweets_set)\n",
    "    shuffle(false_tweets_set)\n",
    "\n",
    "    false_tweets_set = false_tweets_set[:3271]\n",
    "\n",
    "    size = int(0.2 * len(false_tweets_set))\n",
    "\n",
    "    test_set = true_tweets_set[:size] + false_tweets_set[:size]\n",
    "    train_set = true_tweets_set[size:] + false_tweets_set[size:]\n",
    "    return test_set, train_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_test_set, ev_train_set = train_test_split_evoluated(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7675840978593272\n",
      "Most Informative Features\n",
      "               hiroshima = True                1 : 0      =     47.0 : 1.0\n",
      "                 wildfir = True                1 : 0      =     43.7 : 1.0\n",
      "                northern = True                1 : 0      =     38.3 : 1.0\n",
      "                    atom = True                1 : 0      =     31.0 : 1.0\n",
      "                   sever = True                1 : 0      =     28.3 : 1.0\n",
      "                 typhoon = True                1 : 0      =     25.0 : 1.0\n",
      "                 wreckag = True                1 : 0      =     23.0 : 1.0\n",
      "                outbreak = True                1 : 0      =     21.0 : 1.0\n",
      "                 confirm = True                1 : 0      =     19.8 : 1.0\n",
      "             anniversari = True                1 : 0      =     19.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ev_classifier = NaiveBayesClassifier.train(ev_train_set)\n",
    " \n",
    "accuracy = classify.accuracy(ev_classifier, ev_test_set)\n",
    "print(accuracy) \n",
    " \n",
    "print (ev_classifier.show_most_informative_features(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for class True\n",
      "true precision:  0.8770614692653673\n",
      "true recall: 0.8944954128440367\n",
      "true F-measure: 0.8856926570779713\n",
      "\n",
      "Metrics for class False \n",
      "false precision: 0.8923556942277691\n",
      "false recall: 0.8746177370030581\n",
      "false F-measure: 0.8833976833976833\n",
      "\n",
      "Confusion Matrix \n",
      "  |   0   1 |\n",
      "--+---------+\n",
      "0 |<572> 82 |\n",
      "1 |  69<585>|\n",
      "--+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_metrics(ev_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-scores have significantly improved. Now Let's evaluate it more and add the location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add location to the text, so it will appear in the bag of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "text = []\n",
    "for i in range(0, len(data)):\n",
    "    tweet = (data.loc[i].text + \" \" + str(data.loc[i].location) + \" \" + str(data.loc[i].keyword))\n",
    "    text.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data\n",
    "data1.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all nan nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada nan nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California  nan nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school  nan nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                        Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all nan nan   \n",
       "1                                                       Forest fire near La Ronge Sask. Canada nan nan   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...   \n",
       "3                            13,000 people receive #wildfires evacuation orders in California  nan nan   \n",
       "4     Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school  nan nan   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7698776758409785\n",
      "Most Informative Features\n",
      "                 wildfir = True                1 : 0      =     43.7 : 1.0\n",
      "                  bomber = True                1 : 0      =     31.7 : 1.0\n",
      "                   sever = True                1 : 0      =     30.3 : 1.0\n",
      "                 suspect = True                1 : 0      =     23.7 : 1.0\n",
      "                 wreckag = True                1 : 0      =     23.7 : 1.0\n",
      "                 typhoon = True                1 : 0      =     23.7 : 1.0\n",
      "                    raze = True                1 : 0      =     20.3 : 1.0\n",
      "                 rescuer = True                1 : 0      =     19.7 : 1.0\n",
      "                    atom = True                1 : 0      =     17.8 : 1.0\n",
      "                20bomber = True                1 : 0      =     17.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "loc_ev_test_set, loc_ev_train_set = train_test_split_evoluated(data1)\n",
    "\n",
    "loc_ev_classifier = NaiveBayesClassifier.train(loc_ev_train_set)\n",
    " \n",
    "accuracy = classify.accuracy(loc_ev_classifier, loc_ev_test_set)\n",
    "print(accuracy) \n",
    " \n",
    "print (loc_ev_classifier.show_most_informative_features(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for class True\n",
      "true precision:  0.8674698795180723\n",
      "true recall: 0.8807339449541285\n",
      "true F-measure: 0.8740515933232171\n",
      "\n",
      "Metrics for class False \n",
      "false precision: 0.8788819875776398\n",
      "false recall: 0.8654434250764526\n",
      "false F-measure: 0.8721109399075502\n",
      "\n",
      "Confusion Matrix \n",
      "  |   0   1 |\n",
      "--+---------+\n",
      "0 |<566> 88 |\n",
      "1 |  78<576>|\n",
      "--+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_metrics(loc_ev_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This practical work aimed to train and test Naive Bayesian Classifier. There were two phases of evaluation of the model.\n",
    "Text in the first model did not contain any symbols, including hashtags and links. Accuracy was 0.769. F-score for class True was 0.77, for class False 0.76. \n",
    "\n",
    "For the second model hashtags and links were replaced with words hashtag and URL respectively. New accuracy draw up 0.767. F-score improved significantly. F-score for class True was 0.88, for class False 0.88. \n",
    "\n",
    "For the last evaluation, the location and keywords were added. After that F-scores decrease. Keywords and location made the model more complex, however, they did not contain any useful information for the target. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
